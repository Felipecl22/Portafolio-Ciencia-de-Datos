# -*- coding: utf-8 -*-
"""KMeans.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14831Z6SQ6JuCZwE_MnakJfk2OdhJXxkB
"""

from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt

# Generate some random data
X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=42)

# Apply K-Means clustering
kmeans = KMeans(n_clusters=4)
kmeans.fit(X)

# Get cluster centers and labels
centers = kmeans.cluster_centers_
labels = kmeans.labels_

# Plot the data points and cluster centers
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.5)
plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='x')
plt.show()

""" K-Means clustering in simpler terms:

Data Representation:

Imagine you have a bunch of points scattered on a graph, but you don't know anything about them. These points are your data, and you're interested in finding patterns or groups within them.
Choosing K:

Before you start, you need to decide how many groups you want to find. This is represented by 'K'. For example, if you think there might be 3 different groups in your data, then K = 3.
Finding Neighbors:

For each point in your data, the algorithm looks at its 'neighbors' – other points that are close by. It measures the distance between points (like how far apart they are on the graph), usually using something called the Euclidean distance, which is like the straight-line distance between two points.
Assigning Clusters:

Based on these distances, the algorithm groups together points that are close to each other. It tries to create 'clusters' – groups of points that are similar or nearby. Each point is assigned to the cluster that its nearest neighbors belong to.
Iterative Process:

The algorithm repeats this process, adjusting the clusters each time, until they stabilize. It's like moving the points around to find the best groups, and it keeps doing this until it can't improve them anymore.
"""

from sklearn.cluster import KMeans
import numpy as np
import matplotlib.pyplot as plt

# Step 1: Generate some sample data
np.random.seed(0)
X = np.random.rand(100, 2)

# Step 2: Initialize cluster centers
kmeans = KMeans(n_clusters=3, init='random', random_state=42)

# Step 3 and 4: Fit K-Means model to data
kmeans.fit(X)

# Step 5: Get final cluster centers and assignments
centers = kmeans.cluster_centers_
labels = kmeans.labels_

# Step 6: Visualize the clusters
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.5)
plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='x', s=200)
plt.title('K-Means Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

"""In the plot generated by the code:

The small dots represent the data points in the dataset.
The big red X's represent the final cluster centers (centroids) calculated by the K-Means algorithm.
The color of the small dots represents the cluster assignment of each data point. Each cluster is assigned a different color. The assignment is done by the K-Means algorithm based on the proximity of each data point to the cluster centers.

The purpose of the plot is to visualize how the K-Means algorithm has grouped the data points into clusters, with each cluster centered around one of the red X's.

Here's a breakdown of what the plot represents:

Each small dot (data point) belongs to one of the clusters, indicated by its color.
The big red X's represent the final cluster centers (centroids) calculated by the algorithm.
The clusters are formed around these centroids, with each data point assigned to the cluster whose centroid it is closest to.

so the centroids are the points to define the neighorbs based on the distance of this centroid

The centroids (represented by the big red X's in the plot) are the points around which the clusters are formed.
Each data point is assigned to the cluster whose centroid it is closest to, based on some distance metric (typically Euclidean distance).
The centroids play a crucial role in defining the clusters and determining which data points belong to each cluster.
Here's how the process works:

Initialization: Initially, K centroids are randomly placed in the feature space.
Assignment: Each data point is assigned to the cluster whose centroid it is closest to (based on distance).
Update: After all data points have been assigned, the centroids are updated to be the mean (average) of all the data points assigned to their respective clusters.
Repeat: Steps 2 and 3 are repeated iteratively until convergence (i.e., until the centroids no longer change significantly or until a maximum number of iterations is reached).

K are the centroids

In general:

If you set K = 3, you will have 3 centroids and 3 clusters.
If you set K = 5, you will have 5 centroids and 5 clusters.
And so on, depending on the value of K you choose.
The K-Means algorithm aims to partition the data into K clusters by iteratively updating the positions of these centroids and assigning data points to the nearest centroid.

es, exactly! K-Means clustering can be used to segment or group data into clusters based on similarity. This can be applied to various domains, including customer segmentation, where you might want to group customers based on similar attributes or behaviors.

For example, in marketing:

You might have customer data containing features such as age, income, spending habits, etc.
By applying K-Means clustering to this data, you can identify distinct segments or groups of customers who share similar characteristics.
These segments can then be used for targeted marketing strategies, personalized recommendations, or understanding customer preferences.
In the context of people segmentation:

You could have data representing individuals based on various attributes such as age, gender, income, interests, etc.
By clustering this data using K-Means, you can identify different segments of people who share similar characteristics or behaviors.
This can be valuable for market research, customer profiling, or tailoring products/services to specific customer segments.
Overall, K-Means clustering helps to uncover patterns and structures within data, making it easier to understand and analyze complex datasets. It provides insights into how data points are naturally grouped together, allowing for more targeted and effective decision-making in various domains.

The two most common initialization methods for K-Means are:

Random initialization (init='random'):

In this method, K initial cluster centers are selected randomly from the dataset.
Random initialization is simple and fast but may lead to suboptimal clustering results, especially if the initial centroids are chosen poorly.
K-Means++ initialization (init='k-means++'):

In this method, the initial cluster centers are selected using a more sophisticated approach called K-Means++.
K-Means++ aims to spread out the initial centroids to be as far apart from each other as possible, which often leads to better convergence and improved clustering results compared to random initialization.
While K-Means++ initialization tends to yield better results, it may be slightly slower computationally due to the additional calculations involved. However, in practice, the improved clustering quality often outweighs the small increase in computational cost.
"""

from sklearn.cluster import KMeans
import numpy as np
import matplotlib.pyplot as plt

# Step 1: Generate some sample data
np.random.seed(0)
X = np.random.rand(100, 2)

# Step 2: Initialize cluster centers using K-Means++
kmeans = KMeans(n_clusters=3, init='k-means++', random_state=42)

# Step 3 and 4: Fit K-Means model to data
kmeans.fit(X)

# Step 5: Get final cluster centers and assignments
centers = kmeans.cluster_centers_
labels = kmeans.labels_

# Step 6: Visualize the clusters
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.5)
plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='x', s=200)
plt.title('K-Means Clustering with K-Means++ Initialization')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

"""Imagine you have a magical dice that can generate random numbers. Every time you roll this dice, it gives you a different number. However, what if you want the same sequence of random numbers every time you roll the dice?

That's where the concept of a "seed" comes in.

A "seed" is like the starting point for generating random numbers.
When you set a seed, it's like telling the dice to start from a specific position before rolling.
If you set the seed to the same value every time, the sequence of random numbers generated by the dice will be the same each time you roll it.
In programming, setting a random seed works similarly:

When you set a random seed using np.random.seed(), you're telling the random number generator to start from a specific point.
If you set the seed to the same value every time you run your code, you'll get the same sequence of random numbers each time.
This is useful because it allows you to reproduce your results. When you're working on a project or running experiments, having reproducible results is important for debugging, testing, and ensuring consistency.
"""